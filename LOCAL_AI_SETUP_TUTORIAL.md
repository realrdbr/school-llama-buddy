# VollstÃ¤ndige Anleitung: Lokale AI-gestÃ¼tzte Schule-App mit Llama 3.1 8B

## ğŸ¯ Ãœbersicht
Diese Anleitung erklÃ¤rt, wie Sie Ihre Schule-App mit lokalem Llama 3.1 8B AI-Modell, TTS-FunktionalitÃ¤t und KI-gestÃ¼tzter Vertretungsplan-Logik einrichten.

## ğŸ“‹ Voraussetzungen

### 1. Hardware-Anforderungen
- **RAM**: Mindestens 8 GB (16 GB empfohlen fÃ¼r Llama 3.1 8B)
- **Festplatte**: 10 GB freier Speicherplatz
- **CPU**: Moderne Mehrkern-CPU (AI-Berechnungen)

### 2. Software-Voraussetzungen
- Node.js (Version 18+)
- Git
- Moderner Webbrowser mit Web Speech API Support

---

## ğŸš€ Schritt 1: Ollama Installation und Setup

### Windows Installation
```powershell
# Download Ollama von https://ollama.ai/download
# Installiere die .exe Datei
# Oder via Chocolatey:
choco install ollama
```

### macOS Installation
```bash
# Download von https://ollama.ai/download
# Oder via Homebrew:
brew install ollama
```

### Linux Installation
```bash
# Ubuntu/Debian:
curl -fsSL https://ollama.ai/install.sh | sh

# Arch Linux:
yay -S ollama
```

### Llama 3.1 8B Modell herunterladen
```bash
# Ollama starten (falls nicht automatisch gestartet)
ollama serve

# In einem neuen Terminal:
ollama pull llama3.1:8b
```

**âš ï¸ Wichtig**: Das Modell ist ~4.7 GB groÃŸ und kann je nach Internetverbindung lange dauern.

### Ollama-Konfiguration Ã¼berprÃ¼fen
```bash
# Testen ob Ollama lÃ¤uft:
curl http://localhost:11434/api/tags

# Modell testen:
ollama run llama3.1:8b "Hallo, funktioniert das?"
```

---

## ğŸ”§ Schritt 2: Projekt-Setup

### Repository klonen und starten
```bash
# Wenn noch nicht geschehen:
git clone [your-repo-url]
cd school-management-app

# Dependencies installieren:
npm install

# Entwicklungsserver starten:
npm run dev
```

### Supabase-Konfiguration
Die App nutzt bereits konfigurierte Supabase Edge Functions als Proxy fÃ¼r Ollama:
- `ollama-proxy`: Leitet Anfragen an lokales Ollama weiter
- `ai-actions`: FÃ¼hrt KI-gesteuerte Aktionen aus
- Alle CORS-Probleme sind bereits gelÃ¶st

---

## ğŸŒ Schritt 3: Netzwerk-Zugriff einrichten

### Lokales Netzwerk verfÃ¼gbar machen
```bash
# Vite Server fÃ¼r Netzwerk-Zugriff konfigurieren:
npm run dev -- --host 0.0.0.0

# Oder in vite.config.ts hinzufÃ¼gen:
export default defineConfig({
  server: {
    host: '0.0.0.0',
    port: 5173
  }
})
```

### IP-Adresse finden
```bash
# Windows:
ipconfig

# macOS/Linux:
ip addr show
# oder
ifconfig
```

Die App ist dann erreichbar unter: `http://[IHRE-IP]:5173`

### Firewall-Einstellungen
**Windows:**
```powershell
# Port fÃ¼r Vite freigeben:
netsh advfirewall firewall add rule name="Vite Dev Server" dir=in action=allow protocol=TCP localport=5173
```

**Linux (ufw):**
```bash
sudo ufw allow 5173
```

---

## ğŸ—£ï¸ Schritt 4: Text-to-Speech Konfiguration

### Browser-KompatibilitÃ¤t prÃ¼fen
Die App nutzt die Web Speech API (bereits implementiert in `OfflineTTS.tsx`):

**UnterstÃ¼tzte Browser:**
- âœ… Chrome/Chromium (beste QualitÃ¤t)
- âœ… Firefox (eingeschrÃ¤nkt)
- âœ… Safari (macOS/iOS)
- âŒ Ã„ltere Browser

### Deutsche Stimmen installieren

**Windows:**
1. Einstellungen â†’ Zeit und Sprache â†’ Sprache
2. Deutsch hinzufÃ¼gen
3. Sprachpaket â†’ Sprachfunktionen â†’ Text-zu-Sprache herunterladen

**macOS:**
1. Systemeinstellungen â†’ Bedienungshilfen â†’ Gesprochene Inhalte
2. Systemstimme â†’ Anpassen â†’ Deutsch auswÃ¤hlen

**Linux:**
```bash
# Ubuntu/Debian:
sudo apt install espeak-ng-data-de festival-de

# Arch:
sudo pacman -S espeak-ng festival-de
```

### TTS testen
1. Ã–ffnen Sie die App
2. Gehen Sie zu "Audio-Durchsagen"
3. Erstellen Sie eine TTS-Durchsage
4. Klicken Sie "Abspielen"

---

## ğŸ¤– Schritt 5: KI-Vertretungsplan verwenden

### VerfÃ¼gbare KI-Funktionen
Die App unterstÃ¼tzt natÃ¼rlichsprachliche Eingaben fÃ¼r:

**Vertretungsplan-Ã„nderungen:**
- "Herr MÃ¼ller fÃ¤llt morgen aus"
- "Die 10b hat Raumwechsel von 201 nach 305"
- "Frau Schmidt Ã¼bernimmt die Mathestunde in der 9a"
- "Erste Stunde entfÃ¤llt heute"

**AnkÃ¼ndigungen erstellen:**
- "Erstelle eine AnkÃ¼ndigung Ã¼ber den Ausflug"
- "Informiere alle Lehrer Ã¼ber die Konferenz"

### Schritt-fÃ¼r-Schritt Verwendung
1. **KI-Chat Ã¶ffnen**: Navigation â†’ "KI-Assistent"
2. **Eingabe**: Z.B. "Herr KÃ¶nig ist krank, kann jemand die 3. Stunde Mathe in der 10b Ã¼bernehmen?"
3. **KI-Antwort**: Die KI analysiert und schlÃ¤gt eine Vertretung vor
4. **BestÃ¤tigung**: Sie mÃ¼ssen die Ã„nderung bestÃ¤tigen, bevor sie Ã¼bernommen wird
5. **Automatische Erstellung**: Vertretungsplan wird automatisch aktualisiert

---

## ğŸ”’ Schritt 6: Sicherheit und Berechtigungen

### Rollenbasierte Zugriffe
Die App implementiert strenge Berechtigungskontrollen:

**SchÃ¼ler (Level 1-3):**
- âœ… Stundenplan einsehen
- âœ… Vertretungsplan einsehen
- âœ… AnkÃ¼ndigungen lesen
- âŒ Keine KI-Chat Funktionen
- âŒ Keine Verwaltungsfunktionen

**LehrkrÃ¤fte (Level 4-8):**
- âœ… Alle SchÃ¼ler-Rechte
- âœ… AnkÃ¼ndigungen erstellen
- âœ… KI-Chat fÃ¼r AnkÃ¼ndigungen
- âŒ Vertretungsplan bearbeiten
- âŒ Benutzerverwaltung

**Koordination (Level 9):**
- âœ… Alle Lehrer-Rechte
- âœ… Vertretungsplan bearbeiten
- âœ… KI-Chat fÃ¼r Vertretungsplan
- âœ… Klassenverwaltung
- âŒ Benutzerverwaltung

**Schulleitung (Level 10):**
- âœ… Vollzugriff auf alle Funktionen
- âœ… Benutzerverwaltung
- âœ… TTS-Durchsagen
- âœ… Alle KI-Funktionen

### Sichere Datenbank-Konfiguration
Die Datenbank ist bereits mit Row-Level Security (RLS) gesichert:
- Benutzer sehen nur ihre eigenen Chat-VerlÃ¤ufe
- PasswÃ¶rter sind verschlÃ¼sselt
- Berechtigungen werden streng kontrolliert

---

## ğŸ”§ Schritt 7: Troubleshooting

### HÃ¤ufige Probleme

**1. Ollama-Verbindung fehlgeschlagen**
```bash
# PrÃ¼fen ob Ollama lÃ¤uft:
curl http://localhost:11434/api/tags

# Neu starten:
killall ollama
ollama serve
```

**2. Modell nicht gefunden**
```bash
# VerfÃ¼gbare Modelle anzeigen:
ollama list

# Llama 3.1 neu herunterladen:
ollama pull llama3.1:8b
```

**3. TTS funktioniert nicht**
- Browser-Konsole prÃ¼fen (F12)
- Mikrofonberechtigung erteilen
- Deutsche Sprachpakete installieren
- Anderen Browser testen

**4. KI antwortet nicht**
- Ollama-Logs prÃ¼fen: `ollama logs`
- Supabase Edge Function Logs checken
- Internetverbindung prÃ¼fen

**5. Berechtigungsfehler**
- Als Admin anmelden
- Benutzerberechtigung in der Benutzerverwaltung prÃ¼fen
- Browser-Cache leeren

### Performance-Optimierung

**Ollama-Performance:**
```bash
# GPU-Nutzung aktivieren (falls verfÃ¼gbar):
export OLLAMA_GPU=1
ollama serve

# Memory-Limit setzen:
export OLLAMA_MAX_MEMORY=8GB
```

**Browser-Performance:**
- Hardware-Beschleunigung aktivieren
- GenÃ¼gend RAM freigeben
- Andere Tabs schlieÃŸen

---

## ğŸ“± Schritt 8: Mobile Nutzung

### Responsive Design
Die App ist vollstÃ¤ndig responsive und funktioniert auf:
- ğŸ“± Smartphones (iOS/Android)
- ğŸ“± Tablets
- ğŸ’» Laptops
- ğŸ–¥ï¸ Desktop-PCs

### Mobile TTS
**iOS Safari**: Funktioniert nativ
**Android Chrome**: Funktioniert nativ
**Andere Browser**: EingeschrÃ¤nkte UnterstÃ¼tzung

---

## ğŸš€ Schritt 9: Produktions-Deployment

### Lokaler Server (empfohlen)
```bash
# Produktions-Build erstellen:
npm run build

# Mit serve hosten:
npx serve dist -p 80

# Oder mit nginx:
sudo apt install nginx
# nginx konfigurieren fÃ¼r dist/ Ordner
```

### Online-Deployment (optional)
Das Tutorial `DEPLOYMENT_TUTORIAL.md` erklÃ¤rt Lovable-Deployment fÃ¼r Internetzugriff.

---

## ğŸ”„ Schritt 10: Wartung und Updates

### RegelmÃ¤ÃŸige Updates
```bash
# Ollama aktualisieren:
ollama pull llama3.1:8b

# App-Dependencies aktualisieren:
npm update

# Supabase-Migrations anwenden:
# (automatisch bei App-Start)
```

### Backup-Strategie
- **Datenbank**: Supabase erstellt automatische Backups
- **Lokale Daten**: Ollama-Modelle bei Bedarf neu herunterladen
- **Konfiguration**: Git-Repository als Backup

---

## ğŸ“ Support und Hilfe

### Bei Problemen:
1. **Logs prÃ¼fen**: Browser-Konsole (F12)
2. **Ollama-Status**: `ollama ps`
3. **Supabase-Logs**: Im Supabase Dashboard
4. **Community**: GitHub Issues erstellen

### NÃ¼tzliche Befehle
```bash
# System-Status prÃ¼fen:
ollama ps                          # Laufende Modelle
curl localhost:11434/api/tags      # VerfÃ¼gbare Modelle
npm run dev -- --host 0.0.0.0     # Dev-Server fÃ¼r Netzwerk

# Logs anzeigen:
ollama logs                        # Ollama-Logs
tail -f ~/.ollama/logs/server.log  # Detaillierte Logs
```

---

## âœ… Erfolgskriterien

Nach erfolgreicher Einrichtung sollten Sie:

1. âœ… **Login als "Kunadt"** ohne UUID-Fehler
2. âœ… **KI-Chat funktioniert** mit Llama 3.1 8B
3. âœ… **TTS spielt Sprache ab** ohne Fehler
4. âœ… **Vertretungsplan-KI** versteht natÃ¼rliche Eingaben
5. âœ… **Netzwerk-Zugriff** von anderen GerÃ¤ten funktioniert
6. âœ… **Rollenbasierte UI** zeigt nur erlaubte Funktionen

### Test-Checkliste
- [ ] Als verschiedene Benutzertypen anmelden
- [ ] KI-Chat mit Vertretungsplan-Anfrage testen
- [ ] TTS-Durchsage erstellen und abspielen
- [ ] Von Smartphone/Tablet auf App zugreifen
- [ ] Ollama-Verbindung ohne CORS-Fehler

---

**ğŸ‰ Herzlichen GlÃ¼ckwunsch!** 
Ihre lokale AI-gestÃ¼tzte Schule-App ist jetzt vollstÃ¤ndig eingerichtet und einsatzbereit.

Bei Fragen oder Problemen schauen Sie in die Troubleshooting-Sektion oder erstellen Sie ein GitHub Issue.